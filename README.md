# Non-Square_Matrix_Multiplication_CUDA_OpenCL
Optimized and Naive Non-Square Matrix Multiplication With CUDA and OpenCL

Matrix multiplication in both CUDA and OpenCL was implemented, taking advantage of both shared memory and global memory. A matrix of size [M X N] and a second matrix of size [N X M] are initialized. The matrix is then iteratively increased in both the x- and y-dimensions (i.e. [itr*M X itr*N]; [itr*N X itr*M]). First, a naive implementation of matrix multiplication, in both PyCUDA and PyOpenCL, is used, where both matrices are stored in global memory. Second, an optimized implementation of matrix multiplication is used, in both PyCUDA and PyOpenCL, where both matrices are stored in shared memory. Matrices in the naive and optimized PyCuda code were initialized to size 16x14 and 14x16, with each dimension in each matrix was iteratively increased by a factor of 1 to 38. The dimension of the largest matrix was 608 x 608. Matrices in the naive and optimized PyOpenCl code were initialized to the same size as those in PyCuda for consistency. However, matrices in PyOpenCL naive and optimized implementations were able to compute matrices of size 8000x8000 without en- countering memory errors. Matrices in the optimized PyCuda implementation could also compute matrices of this size. Finally, run times were compared for PyCuda, Python (Serial/CPU), and PyOpenCL for both naive and optimized matrix multiplication implementations. PyOpenCL optimized code was clearly able to compute the matrix multiplication much faster than PyOpenCL naive and Python (serial/CPU). PyCuda optimized and naive had similar run-times, with the optimized code barely outperforming the naive implementation.
